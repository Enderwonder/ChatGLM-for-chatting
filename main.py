from transformers import AutoTokenizer, AutoModel
import torch
from memory import MemoryManager, format_vector_snippet # è¨˜æ†¶ç®¡ç†å™¨å‡½æ•¸

# --- é—œéµè©è¨­å®š ---
TRIGGER_KEYWORDS = ["å›æ†¶", "è¨˜å¾—å—", "ä¸Šæ¬¡èªªåˆ°", "é—œæ–¼é‚£ä»¶", "æé†’æˆ‘", "ä¹‹å‰", "è¨˜éŒ„","åå­—","æ„›","å–œæ­¡","è¨å­","æ˜¯"] 
DISTANCE_THRESHOLD = 1.15 #æª¢ç´¢è¨˜æ†¶å‘é‡å€¼

def should_retrieve_memory(text, keywords):
    text_lower = text.lower()
    for keyword in keywords:
        if keyword.lower() in text_lower:
            return True
    return False

# --- ChatGLM æ¨¡å‹è¨­å®š ---
model_path = r"C:\temp_hf_downloads\chatglm6b" # æ¨¡å‹è·¯å¾‘

print(f"ğŸš€ æ­£åœ¨å¾æœ¬åœ°è·¯å¾‘ {model_path} è¼‰å…¥ ChatGLM-6B æ¨¡å‹ä¸­...")
try:
    # ä½¿ç”¨æœ¬åœ°è·¯å¾‘è¼‰å…¥ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    # ä½¿ç”¨æœ¬åœ°è·¯å¾‘è¼‰å…¥æ¨¡å‹
    model = AutoModel.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    #æ¨¡å‹ç§»è‡³GPUåˆ©ç”¨CUDAè¨ˆç®—

    if torch.cuda.is_available():
        model = model.half().cuda() 
        print("ChatGLM-6B æ¨¡å‹å·²ç§»è‡³ GPUã€‚")
    else:
        model = model.float() 
        print("æœªåµæ¸¬åˆ° CUDAï¼ŒChatGLM-6B æ¨¡å‹å°‡åœ¨ CPU ä¸Šé‹è¡Œ (float32)ã€‚")
    model.eval()
    print("ChatGLM-6B æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")
except Exception as e:
    print(f"å¾æœ¬åœ°è·¯å¾‘ {model_path} è¼‰å…¥ ChatGLM-6B æ¨¡å‹å¤±æ•—: {e}")
    print("   è«‹ç¢ºä¿æŒ‡å®šçš„æœ¬åœ°è·¯å¾‘æ­£ç¢ºï¼Œä¸”åŒ…å«æ‰€æœ‰å¿…è¦çš„æ¨¡å‹å’Œ Tokenizer æª”æ¡ˆã€‚")
    exit()


# --- è¨˜æ†¶ç®¡ç†å™¨è¨­å®š ---
try:
    memory_manager = MemoryManager(
        embedding_dim=384, 
        index_file="chat_faiss.idx", 
        memories_pickle_file="chat_text_memories.pkl",
        persistent_text_file="persistent_memories.txt"
    )
    print(f"âœ… è¨˜æ†¶ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸã€‚ç›®å‰æ“æœ‰ {memory_manager.get_total_memories()} æ¢è¨˜æ†¶ã€‚")
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–è¨˜æ†¶ç®¡ç†å™¨å¤±æ•—: {e}")
    print("ç¨‹å¼å°‡ç¹¼çºŒé‹è¡Œï¼Œä½†é•·æœŸè¨˜æ†¶åŠŸèƒ½å¯èƒ½ç„¡æ³•ä½¿ç”¨ã€‚")
    memory_manager = None 

# --- å°è©±è¿´åœˆ ---
history = []
print("\n--- å°è©±é–‹å§‹ (è©³ç´°é™¤éŒ¯æ¨¡å¼) ---")
print("æç¤ºï¼šè¼¸å…¥ '/remember <å…§å®¹>' æ–°å¢è¨˜æ†¶ã€‚")
print("æç¤ºï¼šè¼¸å…¥ '/reload_memories' å¾æª”æ¡ˆé‡è¼‰è¨˜æ†¶ã€‚")
print(f"æç¤ºï¼šåŒ…å«é—œéµè©å¦‚ {TRIGGER_KEYWORDS[:3]}... ç­‰æ™‚ï¼Œæœƒå˜—è©¦å•Ÿç”¨è¨˜æ†¶ã€‚")
print("æç¤ºï¼šè¼¸å…¥ 'çµæŸ' é›¢é–‹ã€‚")

while True:
    input_text = input("\nğŸ‘¤ ä½ æƒ³å°æ¨¡å‹èªªä»€éº¼ï¼š")
    
    if input_text.strip().lower() == "çµæŸ":
        print("âœ… çµæŸå°è©±ï¼Œç¨‹å¼å·²é€€å‡ºã€‚")
        if memory_manager:
            memory_manager.save_memories_on_exit() 
        break

    if input_text.lower().startswith("/remember "):
        if memory_manager:
            text_to_remember = input_text[len("/remember "):].strip()
            if text_to_remember:
                memory_manager.add_memory(text_to_remember) # add_memory å…§éƒ¨æœƒæ‰“å°å‘é‡ç‰‡æ®µ
            else:
                print("âš ï¸ è¨˜æ†¶å…§å®¹ä¸å¯ç‚ºç©ºã€‚")
        else:
            print("ğŸ› ï¸ è¨˜æ†¶ç®¡ç†å™¨æœªæˆåŠŸåˆå§‹åŒ–ï¼Œç„¡æ³•æ–°å¢è¨˜æ†¶ã€‚")
        continue 
    
    if input_text.lower() == "/reload_memories":
        if memory_manager:
            memory_manager.reload_external_memories()
        else:
            print("ğŸ› ï¸ è¨˜æ†¶ç®¡ç†å™¨æœªæˆåŠŸåˆå§‹åŒ–ï¼Œç„¡æ³•é‡è¼‰è¨˜æ†¶ã€‚")
        continue
    # æ–°å¢ï¼šè™•ç†è¨­å®šè§’è‰²çš„æŒ‡ä»¤
    if input_text.lower().startswith("/setrole "):
        persona_description = input_text[len("/setrole "):].strip()
        if persona_description:
            # æ¸…ç©ºæ­·å²ç´€éŒ„ï¼Œä¸¦è¨­å®šæ–°çš„è§’è‰²æ‰®æ¼”åˆå§‹å°è©±
            # "System Note:" åªæ˜¯ç‚ºäº†è®“æˆ‘å€‘é–±è®€æ™‚æ›´æ¸…æ™°ï¼Œæ¨¡å‹æœƒæŠŠå®ƒç•¶ä½œä½¿ç”¨è€…è¼¸å…¥
            # æ¨¡å‹çš„å›ç­” "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†..." ä¹Ÿæ˜¯æˆ‘å€‘è™›æ§‹çš„ï¼Œç›®çš„æ˜¯å½¢æˆä¸€å€‹å®Œæ•´çš„æ­·å²å›åˆ
            history = [
                (f"System Note: ä½ å¾ç¾åœ¨é–‹å§‹å°‡æ‰®æ¼”ä»¥ä¸‹è§’è‰²ï¼š'{persona_description}'ã€‚è«‹ä½ å®Œå…¨æ²‰æµ¸åœ¨é€™å€‹è§’è‰²ä¸­ï¼Œç”¨é€™å€‹è§’è‰²çš„èº«ä»½ã€èªæ°£ã€é¢¨æ ¼å’ŒçŸ¥è­˜å„²å‚™ä¾†é€²è¡Œæ¥ä¸‹ä¾†æ‰€æœ‰çš„å°è©±å’Œå›æ‡‰ã€‚", 
                 "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†æˆ‘çš„æ–°è§’è‰²è¨­å®šã€‚æˆ‘æœƒåŠªåŠ›æ‰®æ¼”å¥½ã€‚")
            ]
            print(f"ğŸ¤– AI è§’è‰²å·²æˆåŠŸè¨­å®šç‚º: {persona_description[:100]}...")
            print("   ï¼ˆå°è©±æ­·å²å·²é‡ç½®ä»¥ä»£å…¥æ–°è§’è‰²ï¼‰")
        else:
            print("âš ï¸ è§’è‰²æè¿°ä¸å¯ç‚ºç©ºã€‚")
        continue # è™•ç†å®ŒæŒ‡ä»¤å¾Œï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡è¼¸å…¥

    # æ–°å¢ï¼šè™•ç†æ¸…é™¤è§’è‰²çš„æŒ‡ä»¤
    if input_text.lower() == "/clearrole":
        history = [] # æ¸…ç©ºæ­·å²ç´€éŒ„ï¼ŒåŒ…æ‹¬è§’è‰²è¨­å®š
        print("ğŸ¤– AI è§’è‰²å·²æ¸…é™¤ï¼Œæ¢å¾©é è¨­å°è©±æ¨¡å¼ã€‚")
        print("   ï¼ˆå°è©±æ­·å²å·²é‡ç½®ï¼‰")
        continue # è™•ç†å®ŒæŒ‡ä»¤å¾Œï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡è¼¸å…¥

    # --- æ•´åˆé•·æœŸè¨˜æ†¶ (åŸºæ–¼é—œéµè©) ---
    retrieved_memories_text_for_prompt = ""
    
    if memory_manager and memory_manager.get_total_memories() > 0:
        # ç¾åœ¨æˆ‘å€‘ç¸½æ˜¯å˜—è©¦æª¢ç´¢è¨˜æ†¶ï¼Œä¸å†åš´æ ¼ä¾è³´ TRIGGER_KEYWORDS ä½œç‚ºå”¯ä¸€è§¸ç™¼
        # é—œéµè©æœªä¾†å¯ä»¥ç”¨æ–¼èª¿æ•´ k å€¼æˆ–é–€æª»å€¼ (å¢å¼·æª¢ç´¢)
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµè© (å¯é¸ï¼Œç”¨æ–¼å¢å¼·æˆ–ç‰¹æ®Šæç¤º)
        keyword_detected = False
        if TRIGGER_KEYWORDS and should_retrieve_memory(input_text, TRIGGER_KEYWORDS):
            keyword_detected = True
            print(f"\nğŸ§  (åµæ¸¬åˆ°é—œéµè©ï¼Œå°‡é€²è¡Œè¨˜æ†¶æª¢ç´¢ä¸¦å¼·åŒ–...)") # æç¤ºç”¨æˆ¶æª¢ç´¢åŸå› 
        else:
            print(f"\nğŸ§  (æ­£åœ¨é€²è¡Œå¸¸è¦è¨˜æ†¶æª¢ç´¢...)")

        # search_memories è¿”å› (texts, query_embedding, distances)
        # å¯ä»¥è€ƒæ…®åœ¨é—œéµè©è§¸ç™¼æ™‚å–å›æ›´å¤šçš„å€™é¸è¨˜æ†¶ï¼Œä¾‹å¦‚ k=5
        num_to_retrieve = 5 if keyword_detected else 3 # é—œéµè©è§¸ç™¼æ™‚æª¢ç´¢æ›´å¤š
        
        initial_relevant_texts, query_embedding, initial_distances = memory_manager.search_memories(input_text, k=num_to_retrieve) 
        
        if query_embedding is not None:
            print(f"    ä½¿ç”¨è€…è¼¸å…¥ \"{input_text[:50]}...\" çš„å‘é‡ (ç¶­åº¦: {query_embedding.shape}):")
            print(f"        {format_vector_snippet(query_embedding)}")
        else:
            print(f"    ç„¡æ³•ç”Ÿæˆä½¿ç”¨è€…è¼¸å…¥ \"{input_text[:50]}...\" çš„å‘é‡ã€‚")

        genuinely_relevant_memories = [] # å„²å­˜é€šéé–€æª»çš„è¨˜æ†¶æ–‡å­—
        if initial_relevant_texts:
            print("    åˆæ­¥æª¢ç´¢åˆ°çš„è¨˜æ†¶ (å°‡æ ¹æ“šç›¸ä¼¼åº¦é–€æª»ç¯©é¸)ï¼š")
            for i, mem_text in enumerate(initial_relevant_texts):
                distance = initial_distances[i]
                if distance < DISTANCE_THRESHOLD:
                    genuinely_relevant_memories.append(mem_text)
                    print(f"    âœ… \"{mem_text[:60]}...\" (L2 è·é›¢: {distance:.4f}) - ç¬¦åˆé–€æª»")
                else:
                    print(f"    âŒ \"{mem_text[:60]}...\" (L2 è·é›¢: {distance:.4f}) - æœªé”é–€æª»")
            
            if genuinely_relevant_memories:
                print(f"    æœ€çµ‚é¸ç”¨ {len(genuinely_relevant_memories)} æ¢ç¬¦åˆé–€æª»çš„è¨˜æ†¶ã€‚")
                facts = "\n".join([f"- {mem}" for mem in genuinely_relevant_memories])
                # ä¿®æ”¹æç¤ºèªï¼Œä½¿å…¶æ›´é€šç”¨
                retrieved_memories_text_for_prompt = (
                    "æ ¹æ“šæˆ‘å€‘çš„å°è©±ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›æˆ‘è¨˜éŒ„çš„ã€èˆ‡ç•¶å‰è©±é¡Œå¯èƒ½ç›¸é—œçš„è³‡è¨Šï¼š\n"
                    f"{facts}\n\n"
                    "è«‹åƒè€ƒé€™äº›è³‡è¨Šï¼Œä¸¦çµåˆæ‚¨çš„åˆ¤æ–·ä¾†å›ç­”å•é¡Œã€‚\n"
                    "å•é¡Œæ˜¯ï¼š\n"
                )
            else:
                print("    â„¹ï¸ é›–åˆæ­¥æª¢ç´¢åˆ°è¨˜æ†¶ï¼Œä½†ç„¡å…§å®¹ç¬¦åˆç›¸ä¼¼åº¦é–€æª»ã€‚")
        else:
            print("    â„¹ï¸ æœªåˆæ­¥æª¢ç´¢åˆ°ä»»ä½•è¨˜æ†¶ã€‚")
            
    # å¦‚æœ genuinely_relevant_memories ç‚ºç©º, retrieved_memories_text_for_prompt æœƒæ˜¯ç©ºå­—ä¸²
    prompt_with_memory = input_text 
    if retrieved_memories_text_for_prompt:
        prompt_with_memory = f"{retrieved_memories_text_for_prompt}\"{input_text}\""
        
    print("ğŸ’¬ æ­£åœ¨ç”Ÿæˆå›æ‡‰...")
    try:
        response, updated_history = model.chat(tokenizer, prompt_with_memory, history=history)
        history = updated_history 
        print("\nğŸ¤– æ¨¡å‹å›ç­”ï¼š", response)
    except Exception as e:
        print(f"âŒ èˆ‡ ChatGLM æ¨¡å‹å°è©±æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("   è«‹æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¢ºè¼‰å…¥ä»¥åŠè¼¸å…¥æ ¼å¼ã€‚")