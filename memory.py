import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import os
import pickle
import torch # ç¢ºä¿ torch å·²åŒ¯å…¥

def format_vector_snippet(vector, n=5):
    """æ ¼å¼åŒ–å‘é‡ç‰‡æ®µä»¥ä¾›é¡¯ç¤º (é¡¯ç¤ºå‰nå€‹å’Œå¾Œnå€‹å…ƒç´ )"""
    if not isinstance(vector, np.ndarray):
        vector = np.array(vector) # ç¢ºä¿æ˜¯ numpy array
    if vector.ndim > 1: # å¦‚æœæ˜¯å¤šç¶­é™£åˆ— (ä¾‹å¦‚æ‰¹æ¬¡ç‚º1çš„æƒ…æ³)
        vector = vector.flatten()
    if vector.size == 0:
        return "[]"
    if vector.size <= 2 * n:
        return f"{vector.tolist()}"
    
    first_part = [f"{x:.4f}" for x in vector[:n]]
    last_part = [f"{x:.4f}" for x in vector[-n:]]
    return f"[{', '.join(first_part)}, ..., {', '.join(last_part)}]"

class MemoryManager:
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2', 
                 embedding_dim=384, 
                 index_file="chat_faiss.idx", 
                 memories_pickle_file="chat_text_memories.pkl",
                 persistent_text_file="persistent_memories.txt"):
        print("ğŸš€ åˆå§‹åŒ–è¨˜æ†¶ç®¡ç†å™¨ (è©³ç´°é™¤éŒ¯æ¨¡å¼)...")
        self.embedding_dim = embedding_dim
        self.index_file = index_file
        self.memories_pickle_file = memories_pickle_file
        self.persistent_text_memories_file = persistent_text_file

        try:
            self.embed_model = SentenceTransformer(model_name)
            if torch.cuda.is_available():
                self.embed_model = self.embed_model.to(torch.device("cuda"))
            print(f"âœ… å¥å­åµŒå…¥æ¨¡å‹ '{model_name}' è¼‰å…¥æˆåŠŸã€‚")
        except Exception as e:
            print(f"âŒ è¼‰å…¥å¥å­åµŒå…¥æ¨¡å‹ '{model_name}' å¤±æ•—: {e}")
            raise

        self.text_memories = []
        self.index = faiss.IndexFlatL2(self.embedding_dim)
        
        self._load_or_rebuild_from_persistent_file()

    def _load_or_rebuild_from_persistent_file(self):
        self.index.reset() 
        self.text_memories = [] 

        if not os.path.exists(self.persistent_text_memories_file):
            print(f"â„¹ï¸ æŒä¹…è¨˜æ†¶æª”æ¡ˆ '{self.persistent_text_memories_file}' ä¸å­˜åœ¨ã€‚å°‡ä»¥ç©ºè¨˜æ†¶å•Ÿå‹•ã€‚")
            self._save_faiss_and_pkl() 
            return

        print(f"ğŸ”„ å¾ '{self.persistent_text_memories_file}' å®Œæ•´é‡å»ºè¨˜æ†¶...")
        loaded_memories_from_text_file = []
        try:
            with open(self.persistent_text_memories_file, 'r', encoding='utf-8') as f:
                for line in f:
                    mem_text = line.strip()
                    if mem_text:
                        loaded_memories_from_text_file.append(mem_text)
            
            unique_memories = []
            seen_memories_set = set()
            for mem in loaded_memories_from_text_file:
                if mem not in seen_memories_set:
                    unique_memories.append(mem)
                    seen_memories_set.add(mem)

            for mem_text in unique_memories:
                self._add_text_to_internal_stores(mem_text, verbose=False) # é‡å»ºæ™‚ä¸éœ€è©³ç´°æ‰“å°æ¯å€‹å‘é‡
            
            self._save_faiss_and_pkl()
            print(f"âœ… å¾ '{self.persistent_text_memories_file}' æˆåŠŸè¼‰å…¥ä¸¦é‡å»º {len(self.text_memories)} æ¢è¨˜æ†¶ã€‚")
        except Exception as e:
            print(f"âŒ å¾ '{self.persistent_text_memories_file}' è¼‰å…¥è¨˜æ†¶æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}ã€‚å°‡ä»¥ç©ºè¨˜æ†¶å•Ÿå‹•ã€‚")
            self.index.reset()
            self.text_memories = []
            self._save_faiss_and_pkl()

    def _add_text_to_internal_stores(self, text_to_remember, verbose=True):
        if not text_to_remember.strip() or text_to_remember in self.text_memories:
            return False 

        try:
            embedding = self.embed_model.encode([text_to_remember], convert_to_numpy=True, normalize_embeddings=True)
            embedding = embedding.astype('float32')
            if embedding.shape[1] != self.embedding_dim:
                print(f"âŒ åµŒå…¥ç¶­åº¦éŒ¯èª¤ for '{text_to_remember[:30]}...'")
                return False
            
            if verbose: # åªæœ‰åœ¨éé‡å»ºæ¨¡å¼ä¸‹ (ä¾‹å¦‚ /remember æŒ‡ä»¤) æ‰æ‰“å°è©³ç´°å‘é‡
                print(f"   å‘é‡åŒ– \"{text_to_remember[:30]}...\" (ç¶­åº¦: {embedding.shape}): {format_vector_snippet(embedding)}")

            self.index.add(embedding)
            self.text_memories.append(text_to_remember)
            return True
        except Exception as e:
            print(f"âŒ æ–°å¢è¨˜æ†¶ '{text_to_remember[:30]}...' åˆ°å…§éƒ¨ FAISS/åˆ—è¡¨æ™‚å‡ºéŒ¯: {e}")
            return False

    def add_memory(self, text_to_remember):
        if not text_to_remember.strip():
            print("âš ï¸ è©¦åœ–æ–°å¢ç©ºç™½è¨˜æ†¶ï¼Œå·²å¿½ç•¥ã€‚")
            return
        
        if text_to_remember in self.text_memories:
            print(f"â„¹ï¸ è¨˜æ†¶ \"{text_to_remember[:50]}...\" å·²å­˜åœ¨æ–¼ç•¶å‰æœƒè©±ï¼Œå¿½ç•¥ã€‚")
            return

        print(f"ğŸ§  æ­£åœ¨ç‚ºæ–°è¨˜æ†¶ç”ŸæˆåµŒå…¥: \"{text_to_remember[:50]}...\"")
        # _add_text_to_internal_stores ç¾åœ¨æœƒæ‰“å°å‘é‡ (å¦‚æœ verbose=True, é è¨­)
        if self._add_text_to_internal_stores(text_to_remember, verbose=True): 
            try:
                with open(self.persistent_text_memories_file, 'a', encoding='utf-8') as f:
                    f.write(text_to_remember + "\n")
                print(f"ğŸ“ æ–°è¨˜æ†¶å·²é™„åŠ åˆ° '{self.persistent_text_memories_file}'ã€‚")
                self._save_faiss_and_pkl()
                print(f"âœ… è¨˜æ†¶æ–°å¢æˆåŠŸ: \"{text_to_remember[:50]}...\" (ç›®å‰ç¸½è¨˜æ†¶æ•¸: {self.index.ntotal})")
            except Exception as e:
                print(f"âŒ å°‡è¨˜æ†¶é™„åŠ åˆ° '{self.persistent_text_memories_file}' æ™‚å‡ºéŒ¯: {e}")
        else:
            print(f"â„¹ï¸ è¨˜æ†¶ \"{text_to_remember[:50]}...\" æœªèƒ½æ–°å¢åˆ°å…§éƒ¨å„²å­˜ã€‚")

    def _save_faiss_and_pkl(self):
        try:
            if self.index.ntotal == len(self.text_memories): 
                faiss.write_index(self.index, self.index_file)
                with open(self.memories_pickle_file, 'wb') as f:
                    pickle.dump(self.text_memories, f)
            else:
                print(f"âš ï¸ è­¦å‘Š: FAISS ç´¢å¼•æ•¸é‡ ({self.index.ntotal}) èˆ‡æ–‡å­—åˆ—è¡¨æ•¸é‡ ({len(self.text_memories)}) ä¸ç¬¦ï¼Œæœªå„²å­˜ .idx/.pklã€‚")
        except Exception as e:
            print(f"âŒ å„²å­˜ FAISS/PKL ç›¸é—œè¨˜æ†¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def save_memories_on_exit(self):
        print(f"ğŸ’¾ ç¨‹å¼é€€å‡ºï¼Œç¢ºèªè¨˜æ†¶ç‹€æ…‹...")
        self._save_faiss_and_pkl()
        print(f"âœ… è¨˜æ†¶ç‹€æ…‹å·²å„²å­˜ã€‚ç¸½å…± {self.index.ntotal} æ¢è¨˜æ†¶ã€‚")

    def search_memories(self, query_text, k=3):
        """
        æœå°‹èˆ‡æŸ¥è©¢æ–‡å­—æœ€ç›¸é—œçš„ k æ¢è¨˜æ†¶ã€‚
        è¿”å›: (retrieved_texts, query_embedding, distances)
              æˆ–åœ¨éŒ¯èª¤/ç„¡çµæœæ™‚è¿”å› ([], None, [])
        """
        if not query_text.strip() or self.index.ntotal == 0:
            return [], None, []
        try:
            query_embedding = self.embed_model.encode([query_text], convert_to_numpy=True, normalize_embeddings=True)
            query_embedding = query_embedding.astype('float32')

            if query_embedding.shape[1] != self.embedding_dim:
                 print(f"âŒ éŒ¯èª¤ï¼šæŸ¥è©¢åµŒå…¥ç¶­åº¦ ({query_embedding.shape[1]}) èˆ‡ FAISS æœŸæœ›ç¶­åº¦ ({self.embedding_dim}) ä¸ç¬¦ã€‚")
                 return [], None, []

            actual_k = min(k, self.index.ntotal)
            if actual_k == 0: return [], query_embedding, [] # è¿”å›æŸ¥è©¢å‘é‡ï¼Œå³ä½¿æ²’æœ‰çµæœ
                
            distances, indices = self.index.search(query_embedding, actual_k)
            
            retrieved_texts = [self.text_memories[i] for i in indices[0]]
            retrieved_distances = distances[0] # å–å¾—å°æ‡‰çš„è·é›¢é™£åˆ—

            return retrieved_texts, query_embedding, retrieved_distances
        except Exception as e:
            print(f"âŒ æœå°‹è¨˜æ†¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return [], None, []

    def get_total_memories(self):
        return self.index.ntotal

    def reload_external_memories(self):
        print(f"ğŸ”„ æŒ‡ä»¤è§¸ç™¼ï¼šæº–å‚™å¾ '{self.persistent_text_memories_file}' é‡æ–°è¼‰å…¥ä¸¦é‡å»ºæ‰€æœ‰è¨˜æ†¶...")
        self._load_or_rebuild_from_persistent_file()

if __name__ == '__main__':
    print("--- MemoryManager è©³ç´°é™¤éŒ¯æ¨¡å¼æ¸¬è©¦ ---")
    if os.path.exists("test_faiss.idx"): os.remove("test_faiss.idx")
    if os.path.exists("test_memories.pkl"): os.remove("test_memories.pkl")
    if os.path.exists("test_persistent_memories.txt"): os.remove("test_persistent_memories.txt")

    manager = MemoryManager(index_file="test_faiss.idx", 
                            memories_pickle_file="test_memories.pkl", 
                            persistent_text_file="test_persistent_memories.txt")
    
    print(f"\nåˆå§‹è¨˜æ†¶æ•¸é‡: {manager.get_total_memories()}")
    manager.add_memory("æˆ‘æœ€å–œæ­¡çš„é¡è‰²æ˜¯è—è‰²ã€‚")
    manager.add_memory("æˆ‘ä½åœ¨å°åŒ—ã€‚")
    
    query = "ä½ å–œæ­¡ä»€éº¼é¡è‰²ï¼Ÿ"
    print(f"\nğŸ” æ¸¬è©¦æœå°‹: \"{query}\"")
    texts, q_embed, dists = manager.search_memories(query, k=1)
    print(f"   æŸ¥è©¢å‘é‡ (ç¶­åº¦: {q_embed.shape if q_embed is not None else 'N/A'}): {format_vector_snippet(q_embed) if q_embed is not None else 'N/A'}")
    if texts:
        for i, txt in enumerate(texts):
            print(f"   -> ç›¸é—œè¨˜æ†¶: \"{txt}\", L2è·é›¢: {dists[i]:.4f}")
    else:
        print("   -> æœªæ‰¾åˆ°ç›¸é—œè¨˜æ†¶ã€‚")

    manager.reload_external_memories() # ç¢ºä¿ reload ä¹Ÿæ­£å¸¸
    print(f"é‡æ–°è¼‰å…¥å¾Œè¨˜æ†¶æ•¸é‡: {manager.get_total_memories()}")

    if os.path.exists("test_faiss.idx"): os.remove("test_faiss.idx")
    if os.path.exists("test_memories.pkl"): os.remove("test_memories.pkl")
    if os.path.exists("test_persistent_memories.txt"): os.remove("test_persistent_memories.txt")
    print("\nâœ… è©³ç´°é™¤éŒ¯æ¨¡å¼æ¸¬è©¦å®Œæˆä¸¦æ¸…ç†æ¸¬è©¦æª”æ¡ˆã€‚")